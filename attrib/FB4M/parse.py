import os
import re
import gc
import string
import cPickle

import numpy as np
import scipy.sparse as sp
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


def save_sparse_csr(filename, array):
    np.savez_compressed(filename, data=array.data, indices=array.indices,
                        indptr=array.indptr, shape=array.shape)

def load_sparse_csr(filename):
    loader = np.load(filename)
    return sp.csr_matrix((loader['data'], loader['indices'], loader['indptr']),
                         shape=loader['shape'])


# Put the freebase4M data absolute path here
datapath = '/mfs/jiaxin/data/freebase-has-desc/'
rand_split_path = os.path.join(datapath, 'rand-split') + '/'

assert datapath is not None

if 'data' not in os.listdir('../'):
    os.mkdir('../data')


def parseline(line):
    lhs, rel, rhs = line.split('\t')
    lhs = lhs.split(' ')
    rhs = rhs.split(' ')
    rel = rel.split(' ')
    return lhs, rel, rhs

entity2idx = {}
idx2entity = {}

#############################################################################
### Creation of the word dictionaries and bag-of-words feature for text

pjoin = os.path.join
pdir = os.path.dirname
pabs = os.path.abspath
ENTITY_DESCRIPTION_DATA = pjoin(datapath, 'topic-description-has-desc.tsv')

items = []
with open(ENTITY_DESCRIPTION_DATA, 'r') as f:
    text = f.read().decode('utf8').strip().split('\n')
for line in text:
    arr = line.split('\t')
    item = {'mid': arr[0].strip(), 'description': arr[1].strip()}
    items.append(item)

stopwords_ = set(stopwords.words())
punctuations = set(string.punctuation)
items_seg = []
for item in items:
    mid = item['mid']
    desc = item['description']

    # letters only
    desc = re.sub("[^a-zA-Z]", " ", desc)

    # discuss: whether to deal with stopwords
    words = word_tokenize(desc.lower())
    # words = filter(lambda x: x not in punctuations, words)
    words = filter(lambda x: x not in stopwords_, words)

    items_seg.append((mid, words))

del items
del text
gc.collect()

############################################################################
### Creation of the n-gram dictionaries and bag-of-ngrams feature for text
### ngrams are generated by word with start and ending marks (#)

n = 3
indptr = [0]
indices = []
data = []

ngram2id = {}
id2ngram = {}
for mid, words in items_seg:
    if mid not in entity2idx:
        idx = len(entity2idx)
        entity2idx[mid] = idx
        idx2entity[idx] = mid
        for word in set(words):
            word = '#%s#' % word
            for i in xrange(len(word) - n + 1):
                ngram = word[i:(i+n)]
                id_ = ngram2id.setdefault(ngram, len(ngram2id))
                id2ngram.setdefault(id_, ngram)
                indices.append(id_)
                data.append(1)
        indptr.append(len(indices))

entity_ngrams = sp.csr_matrix((data, indices, indptr), dtype='float32').log1p()
print 'entity_ngrams.shape:', entity_ngrams.shape

print 'len(entity2idx):', len(entity2idx)
print 'len(idx2entity):', len(idx2entity)

# for k in sorted(ngram2id.keys()):
#     print k,
# print

print 'len(ngram2id):', len(ngram2id)
print 'len(id2ngram):', len(id2ngram)

with open('../data/FB4M_%dgram2id.pkl' % n, 'w') as f:
    cPickle.dump(ngram2id, f, -1)
with open('../data/FB4M_id2%dgram.pkl' % n, 'w') as f:
    cPickle.dump(id2ngram, f, -1)
save_sparse_csr('../data/FB4M-bag-of-%dgrams.npz' % n, entity_ngrams)

#################################################
### Creation of the entities/indices dictionnaries

np.random.seed(753)

entleftlist = []
entrightlist = []
rellist = []

for datatyp in ['train', 'valid', 'test']:
    f = open(rand_split_path + '%s.tsv' % datatyp, 'r')
    dat = f.readlines()
    f.close()
    for i in dat:
        lhs, rel, rhs = parseline(i[:-1])
        entleftlist += [lhs[0]]
        entrightlist += [rhs[0]]
        rellist += [rel[0]]

entleftset = np.sort(list(set(entleftlist) - set(entrightlist)))
entsharedset = np.sort(list(set(entleftlist) & set(entrightlist)))
entrightset = np.sort(list(set(entrightlist) - set(entleftlist)))
relset = np.sort(list(set(rellist)))

nbleft = len(entleftset)
nbshared = len(entsharedset)
nbright = len(entrightset)
print "# of only_left/shared/only_right entities: ", nbleft, '/', nbshared, '/', nbright
# add relations at the end of the dictionary
idx = len(entity2idx)
for i in relset:
    entity2idx[i] = idx
    idx2entity[idx] = i
    idx += 1
nbrel = idx - len(entity2idx)
print "Number of relations: ", nbrel

f = open('../data/FB4M_entity2idx.pkl', 'w')
g = open('../data/FB4M_idx2entity.pkl', 'w')
cPickle.dump(entity2idx, f, -1)
cPickle.dump(idx2entity, g, -1)
f.close()
g.close()

#################################################
### Creation of the dataset files

unseen_ents=[]
remove_tst_ex=[]

for datatyp in ['train', 'valid', 'test']:
    print datatyp
    f = open(rand_split_path + '%s.tsv' % datatyp, 'r')
    dat = f.readlines()
    f.close()

    # Declare the dataset variables
    inpl = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)),
            dtype='float32')
    inpr = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)),
            dtype='float32')
    inpo = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)),
            dtype='float32')
    # Fill the sparse matrices
    ct = 0
    for i in dat:
        lhs, rel, rhs = parseline(i[:-1])
        if lhs[0] in entity2idx and rhs[0] in entity2idx and rel[0] in entity2idx: 
            inpl[entity2idx[lhs[0]], ct] = 1
            inpr[entity2idx[rhs[0]], ct] = 1
            inpo[entity2idx[rel[0]], ct] = 1
            ct += 1
        else:
            if lhs[0] in entity2idx:
                unseen_ents+=[lhs[0]]
            if rel[0] in entity2idx:
                unseen_ents+=[rel[0]]
            if rhs[0] in entity2idx:
                unseen_ents+=[rhs[0]]
            remove_tst_ex+=[i[:-1]]

    # Save the datasets
    if 'data' not in os.listdir('../'):
        os.mkdir('../data')
    f = open('../data/FB4M-%s-lhs.pkl' % datatyp, 'w')
    g = open('../data/FB4M-%s-rhs.pkl' % datatyp, 'w')
    h = open('../data/FB4M-%s-rel.pkl' % datatyp, 'w')
    cPickle.dump(inpl.tocsr(), f, -1)
    cPickle.dump(inpr.tocsr(), g, -1)
    cPickle.dump(inpo.tocsr(), h, -1)
    f.close()
    g.close()
    h.close()

unseen_ents=list(set(unseen_ents))
print len(unseen_ents)
remove_tst_ex=list(set(remove_tst_ex))
print len(remove_tst_ex)

# for i in remove_tst_ex:
#     print i


